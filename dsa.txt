MISC
# Is the data ordered? If so, you have advantages: (binary search)
constraints make things able to be fast

BINARY SERACH
must have sorted data
make sure to make the lo inclusive and the hi exclusive
uses a while loop to crunch down on hi and low
Takes in a needle and haystack
set hi and lo and midpoint
mid gets recalculated on every pass
don't forget to use floor on the init mid calc
don't forget to use mid as an index of haystack

BUBBLE SORT
runtime is O(n^2)
goes throuh arr 2 index at a time and compares them.
On each pass, the highest number gets put at the end, so you can decr the end of the array index since
you don't need to look at it anymore
subtract i from inner loop conditional
also, subtract 1 from inner loop conditional
Uses 2 loops, one to go through array, the other to look and do the swapping.

LINKED LIST
aka node-based ds
 Order of ops is important, can't set C.next to undefined, otherwise you could never access D again. Use temp var to swap.
 Anthing dealing with the ends is constant time. Dealing with the middle can be costly if traversal is large
 Recommend avoiding traversal
tldr; update your len, update your pointers, check for null and ends of list

QUEUE
a specific implemenation of a a linkned-list FIFO
create node type. fn has length, head, tail, constructor, enqueue, deque, peek
don't forget to keep track of the length
enque:
     update length
     create node
     check for null and set both tail and head to new node.
 else: store tail and update pointer and old tail next.
dequeue:
     check for null
     update length
     store head and move pointer
     if len=0, update tail too
     return val
peek:
     check for null and return
gotchas: updated len, temp store head or tail, empty queue


STACK
opp of queue. Pointers point backwards, head is at the end


RING BUFFER
Like an array with null to the left and right of the head and the tail. 
Block of values can shift around throught the array. 
Tail can even go off the edge and wrap around to the beginning (using modulo)


RECURSION
base case first, then recurse!

refun(n) {
    if( n===1 ) return 1
    return n + refun( n-1 )
}

3 parts to recursion
pre: return n +
recurse: refun( n-1 )
post: return 1

don't use the base case in the recursion
use it when you can't use a for loop (no defined end, or has a branching factor)
They go down and then back up. Fn calls stay open until the inner ones return. 


QUICK SORT
runtime is nlog(n) to X^2
Creates multiple pivot points to keep sorting smaller arrays
both hi and lo are inclusive
start off of the lo
nLogN, but could be n^2 in the worst possible case
2 funcs: one to partition and one to sort; partition return the pivot point
partition fn return the pivot index
base case: when lo is equal or greater than hi

TREE
these are known as depth-first traversals, they preserve the shape of the tree
these are all just pushing and popping functions on a stack
3 types of traversal:   
    pre order:
        Visit Node (print value)
        Recurse Left
        Recurse Right
    in order:   
        Recurse Left
        Visit Node (print value)
        Recurse Right
    post order:
        Recurse Left
        Recurse Right
        Visit Node (print value)


BREADTH-FIRST SEARCH (TREE)
uses a queue


BINARY SEARCH TREE
one rule is applied to every node: left side is <= and right is >
finding or inserting runtime is log(n) to O(n)
    measured in terms of the height of the tree


HEAP (Priority Queue)
Binary tree where every child is either smaller (max heap) or larger (min heap); these are the heap conditions
You don't traverse this tree, use an array to keep track of tree indices
Heaps maintain a weak ordering
Heaps are always balanced, complete on each level, no gaps
Deleting removes the top node, inserting adds to the bottom and bubbles up
    Delete top node and replace with last node, then heapify down to it's place.
when indexing the nodes to an array, can calculate child indexes using 2i+1 and 2i+2. Get parent with (i-1)/2
helper functions for getting parent and child indices
length is used for insertion and deletion
runtime: log(n)


TRIE TREE
Often used for caching or auto-complete. Like a tree or graph of letters that create words. 
Insertion and deletion go up and down nodes to check if child is the next letter of given string, for ex. 


GRAPHS
cycle: visiting 3 or more nodes and returning to original
acyclic: graph that contains no cycles
connected: when every node has a path to another node
directed: when paths have one way direction
undirected: both way directions
node: vertex, point
weighted: connections have associated weight
edge: connection
runtime is O(V * E)

2 main types: Adjency list and matrix(less common)
List is array of nodes. Nodes have array of edges and weights. 

BFS on a graph
returns a path
uses prev and seen arrays to manage path. prev keeps track of what node put another node in the queue. o
must build out the path to get to needle after going through the graph

DFS on a graph
recursion
O(E+V)


DIJKSTRA'S SHORTEST PATH
Gives you the shortest path from one node to every other node in the graph.
uses BFS
uses a prev list
can't use negative weighs
runtime: O(V^2 + E)


MAPS
load factor: amount of data in map vs. max capacity
key: hashable value that points to value. 
value: 
collision: 2 keys map to same cell

ideal load factor is 0.7
uses hashing algo to find place to put key:value into the array:  key -> hash code -> index of array
if collisions occur, they use different ways to move to the next available slot: chaining


LRU - Least Recently Used
caching mechanism that will evict the least recently used item
uses a doubly linked list
Runtime: O(1)
Must remember to update your lookup an revLookup when trimming cache.





